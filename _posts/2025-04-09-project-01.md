---
title: "Project 01"
date: "2025-04-09 00:00:00 +0900"
categories: [Project, one]
tags: [Twitter, Scraping,twitterapi.io]
---

# プロジェクトベースで進めていく

- SNS（Twitter, Instagram, TikTok など）を監視し、特定ユーザーの投稿・リツイート・いいね等の情報を収集・データベース化して管理

## 概要


- 背景：
  - 全自動で特定のアカウントを監視したい。
  - プロジェクトとして開発し、運用してみたい

- プロジェクト概要
  - プロジェクト名：WatchDog
  - 目的：
    - SNS（Twitter, Instagram, TikTok など）を監視し、特定ユーザーの投稿・リツイート・いいね等の情報を収集・データベース化して管理
    - 集めたデータを分析
  - 目標：
    - アカウント自動運用: 収集 → AI → 投稿 → 計測
    - 対象分析：
      - 関係値数値化
      - ツイート：センチメント
  - 

## 1. PostgreSQLをUbuntuサーバーに構築

- 目的：ここに取得したデータを保存

- インストール：
  - パッケージの更新
    - `sudo apt update && sudo apt upgrade -y`
  - PostgreSQLのインストール
    - `sudo apt install postgresql postgresql-contrib -y`
  - サービスの起動確認
    - `sudo systemctl status postgresql`
  - PostgreSQLユーザーに切り替え
    - `sudo -i -u postgres`
    - なぜか：
      - PostgreSQLをインストールすると、OS上に postgres という専用ユーザーが自動で作られる。
      - このユーザーだけが、初期状態でPostgreSQLの操作権限を持っている
      - 例えば：
        - データベースの作成
        - ユーザー（ロール）の作成
        - 設定の変更
      - これらの「DB管理者としての操作」は、このpostgresユーザーで行うのが基本
- 初期設定：
  - ログイン：`sudo -u postgres psql`
  - PW設定：`ALTER USER postgres WITH PASSWORD 'New_PW';`
  - 認証情報変更：`sudo nano /etc/postgresql/16/main/pg_hba.conf`
    - `local   all    postgres     md5`
  - 再起動：`sudo systemctl restart postgresql`
  - ログイン：`psql -U postgres`
    - 使えるコマンド：
      - `\c`: 接続中のユーザーとDBの確認
      - `\l`: DBをリストで表示
      - `\du`: ユーザー（ロール）の一覧を表示
- プロジェクト用のユーザーとデータベースを作成：
  - PostgreSQLユーザー watchdog_user 作成: `CREATE USER watchdog_user WITH PASSWORD 'your_secure_password';`
  - データベース watchdog 作成: `CREATE DATABASE watchdog OWNER watchdog_user;`
  - 権限の確認と付与: `GRANT ALL PRIVILEGES ON DATABASE watchdog TO watchdog_user;`
    - DB作成時に OWNER にすれば基本的にそのユーザーが全権限持つけど、念のため
  - 一度抜ける：`\q`
  - 認証情報の変更：`sudo nano /etc/postgresql/16/main/pg_hba.conf`
    - `local   all    watchdog_user     md5`追加
  - 再起動：`sudo systemctl restart postgresql`
  - watchdog_userとしてDB(watchdog)に接続：`psql -U watchdog_user -d watchdog`

- DBを構築：SNSごとにスキーマを作る（各SNS専用の名前空間）
  - 特徴
    - SNSごとに「フォルダ」のように分けて整理できる
    - SNSごとにスキーマをエクスポート・管理しやすい
    - アプリからアクセスする時に名前空間を明示できる（例: twitter.tweets）
    - スケーラビリティ・将来的な拡張性が高い（例: 権限分離・マルチSNS）
  - スキーマ作成: `CREATE SCHEMA twitter AUTHORIZATION watchdog_user;`
    - `twitter`というスキーマを作成した。
  - スキーマ確認：`\dn`

- テーブルを作成
  - APIで取得できるデータを参考にテーブルを作成していく
  - 使えるコマンド：
    - 特定のスキーマ(`twitter`)内のテーブル名などを確認：`\dt twitter.*`
    -  テーブル(`twitter.users`)の中身を確認: `SELECT * FROM twitter.users;`
      - 見ずらい時は拡張表示モード`\x`
  - テーブル：`twitter.users`
    - 利用するAPI：`https://docs.twitterapi.io/api-reference/endpoint/get_user_by_username`
    - 作成：`CREATE TABLE twitter.users (id TEXT PRIMARY KEY,...);`
    - テーブル名変更：`ALTER TABLE twitter.testusers RENAME TO user_info;`

- DBの接続できるように設定
  - `sudo nano /etc/postgresql/16/main/postgresql.conf`
    - listen_addresses は、PostgreSQLがどのIPアドレスで接続待機するかを決めます。
      - `listen_addresses = '192.168.1.110'`
  - `sudo nano /etc/postgresql/16/main/pg_hba.conf`
    - pg_hba.conf は、どのIPアドレスからの接続を許可するかを決めます。
      - `host    all    all    192.168.1.170/32    scram-sha-256`
  - 再起動：`sudo systemctl restart postgresql`

## 2. プロジェクト用のディレクトリ構成

- ディレクトリ構成:  

```pgsql
scripts/
└── projects/
    └── watchdog/
        └── twitter/
            ├── __init__.py
            ├── config.py               ← APIキーや共通設定
            ├── client.py               ← API呼び出し専用クラス
            ├── models/
            │   ├── __init__.py
            │   └── user.py             ← データ構造定義（pydantic等）
            ├── db/
            │   ├── __init__.py
            │   └── insert_user.py      ← DB挿入専用関数
            ├── fetch/
            │   ├── __init__.py
            │   └── fetch_user_info.py  ← 実行スクリプト：取得+保存
            └── tests/
                ├── test_client.py
                └── test_insert.py
```

- プロジェクト単位の開発方法
  - 今回はPythonの仮想環境（venv）を使う
    - 仮想環境の作成: `python -m venv venv`
    - 仮想環境を有効化: `.\venv\Scripts\Activate`
    - ここに`requirements.txt`を作ってライブラリを入れていく
      - `pip install -r requirements.txt`

### 2.1 IDからユーザーの情報を取得

- DB内にテーブルを用意：`CREATE TABLE twitter.user_info (id TEXT PRIMARY KEY,...);`
- ファイルと役割：
  - client.py: APIリクエストを担当（データを取得）
  - `insert_user.py`: ユーザー情報をPostgreSQLの twitter.user_info テーブルに挿入。
  - `fetch_user_info.py`: 指定したTwitterユーザーのプロフィール情報をAPIから取得し、DBに保存。
- 結果：ユーザーデータを取得し、テーブルに保存出来た。

### 2.2 DBからユーザーIDを取得し、全てのツイートを取得（投稿のみ）

- ファイルと役割：
  - `insert_user_tweets.py`: 取得したツイートを twitter.user_tweets テーブルに挿入
  - `fetch_user_tweets.py`: 指定ユーザーのツイートをAPIからページネーションで取得。
- 結果：成功。とりあえず、IDは`fetch_user_tweets.py`: にハードコードしてる。

### 2.3 取得したツイートを分析

- ファイルと役割：
  - `export_top_posts.py`: TOP10をcsvに出力
- ここまででこのAPI用いたデータ取得→分析を終了
- 全てのツイートの情報を使いたいので、`advanced search`のAPIを利用する

### 2.4 全てのツイートを取得（投稿、引用リツイート、返信すべて）

- DBにテーブル作成: 
  - 確認：
    - `\x`
    - `SELECT * FROM twitter.tweets_test;`

- 特定の人物のツイートデータを全て取得しjsonファイルに保存：
  - fetch_user_tweets_all_page.py: `C:\Users\ebisu\Nextcloud\Scripts\Project_01\twitter\fetch\fetch_user_tweets_all_page.py`
  - wrap_response_data.py:  `C:\Users\ebisu\Nextcloud\Scripts\Project_01\twitter\wrap_response_data.py`
    - jsonファイルの中身を少し整える

- 実行順番：main.py → patch_incomplete.py → patch_incomplete_01.py の順で実行される際の簡潔な流れです：
  - main.py: `C:\Users\ebisu\Nextcloud\Scripts\Project_01\twitter\project_root\main.py`
    - 上記で取得したデータを元にそれをDBのテーブルに追加していく。
    - ツイートデータのIDをデータベースから取得。
    - 不完全なツイートのID（tweet_id）を取得して、それを patch_incomplete.py で処理するために渡す。
  - patch_incomplete.py: `C:\Users\ebisu\Nextcloud\Scripts\Project_01\twitter\project_root\patch_incomplete.py`
    - 各ツイートIDに対して、APIを使ってツイートデータを取得。
    - 取得したツイートデータを必要な情報（テキスト、返信、引用、著者など）を抽出し、patch_incomplete_01.py に渡して更新処理を行う
  - patch_incomplete_01.py: `C:\Users\ebisu\Nextcloud\Scripts\Project_01\twitter\project_root\patch_incomplete_01.py`
    - 深いツイート（返信、引用）について再帰的に取得し、ツイートデータをデータベースに更新。
    - API呼び出し時にキャッシュを利用し、すでに取得済みのツイートが重複しないようにする。
  - 最終的に、ツイートデータが不完全な状態から更新され、完全なデータがデータベースに格納される。

- テーブルをjsonファイルに抽出
  - tabele_to_json.py: `C:\Users\ebisu\Nextcloud\Scripts\Project_01\twitter\project_root\tabele_to_json.py`

- 上記のjsonファイルからChatgptのapiを使って思想分析
  - tweet_id_specify_analysis.py: `C:\Users\ebisu\Nextcloud\Scripts\Project_01\twitter\analysis\tweet_id_specify_analysis.py`
  - tweet_id_specify_analysis_deep.py: `C:\Users\ebisu\Nextcloud\Scripts\Project_01\twitter\analysis\tweet_id_specify_analysis_deep.py`





### 2.5 ツイートの分析

- テスト：DBのテーブル`tweets`から`text`を抽出し頻出語彙を抜き出す
  - 前処理：
    - 必要なデータをDBのテーブルから抜き出して`csv`形式で保存：`extract_tweets.py`
    - 分析の際に不必要な文字を消して綺麗にする：`preprocess_tweet_text.py`
  - 分析：
    - トピックと頻出語彙の自動抽出（統計的分析）: `topic_keywords.py`
- 


## 2. Dockerを使ってn8nをセットアップ

- Dockerのインストール（Ubuntuの場合）
  - `sudo apt update`
  - `sudo apt install -y docker.io docker-compose`
  - `sudo systemctl enable --now docker`
    - これでDockerとdocker-composeがインストールされて、Dockerが自動起動するようになる
  - `sudo usermod -aG docker $USER`: bentham を docker グループに追加
  - `docker ps`: 動いているか確認
- n8n用のディレクトリを作成
  - `mkdir ~/n8n`
  - `cd ~/n8n`
  - `mkdir ./data`
  - `sudo chown -R $USER:$USER ./data`
  - `sudo chmod -R 755 ./data`
- docker-compose.yml を作成
  - `nano docker-compose.yml`
  - 中身は省略
- n8nを起動
  - `docker-compose up -d`